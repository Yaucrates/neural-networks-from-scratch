{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc27c90-c028-4fbd-a990-b25f8b9e4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from solutions.mnistdata import MnistData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab18a96-e768-4f3f-b5da-5a935b0275d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given layer sizes.\n",
    "        \n",
    "        Parameters:\n",
    "        - sizes: A list of integers where each integer represents the number of neurons in that layer.\n",
    "               The first element is the input layer size, the last is the output layer size,\n",
    "               and any in between are hidden layers.\n",
    "               \n",
    "        Initialize:\n",
    "        - layer_count: Total number of layers\n",
    "        - sizes: Store the layer sizes\n",
    "        - biases: Random initial biases for each layer except the input layer\n",
    "        - weights: Random initial weights between each pair of adjacent layers\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        Implement the sigmoid activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        - z: Input value or numpy array\n",
    "        \n",
    "        Returns:\n",
    "        - Output of sigmoid function: 1 / (1 + e^(-z))\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def activation_function_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implement the derivative of the sigmoid activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        - z: Input value or numpy array\n",
    "        \n",
    "        Returns:\n",
    "        - Derivative of sigmoid function: sigmoid(z) * (1 - sigmoid(z))\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def feed_forward(self, a):\n",
    "        \"\"\"\n",
    "        Compute the output of the network given an input.\n",
    "        \n",
    "        Parameters:\n",
    "        - a: Input to the network (first layer activations)\n",
    "        \n",
    "        Returns:\n",
    "        - The output activations of the final layer after passing through each layer\n",
    "          and applying the activation function\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate the network's performance on test data.\n",
    "        \n",
    "        Parameters:\n",
    "        - test_data: List of tuples (x, y) where x is input and y is expected output\n",
    "        \n",
    "        Returns:\n",
    "        - Count of test samples correctly classified by the network\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the cost function with respect to the output activations.\n",
    "        For quadratic cost, this is simply the difference between output and expected output.\n",
    "        \n",
    "        Parameters:\n",
    "        - output_activations: The actual output from the network\n",
    "        - y: The expected/target output\n",
    "        \n",
    "        Returns:\n",
    "        - The gradient of the cost function with respect to the output activations\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        - training_data: List of tuples (x, y) where x is input and y is expected output\n",
    "        - epochs: Number of training epochs (iterations over the entire training data)\n",
    "        - mini_batch_size: Size of each mini-batch for gradient descent\n",
    "        - eta: Learning rate\n",
    "        - test_data: Optional test data to evaluate performance after each epoch\n",
    "        \n",
    "        This method should:\n",
    "        1. Shuffle the training data for each epoch\n",
    "        2. Split training data into mini-batches\n",
    "        3. Call update_mini_batch on each mini-batch\n",
    "        4. If test_data is given, it should evaluate and print the network's performance after each epoch\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying gradient descent\n",
    "        using backpropagation to a single mini batch.\n",
    "        \n",
    "        Parameters:\n",
    "        - mini_batch: List of tuples (x, y) where x is input and y is expected output\n",
    "        - eta: Learning rate\n",
    "        \n",
    "        This method should:\n",
    "        1. Initialize gradient arrays for biases and weights\n",
    "        2. For each training example in the mini-batch:\n",
    "           - Call backpropagation to get gradients\n",
    "           - Add these gradients to the total\n",
    "        3. Update weights and biases by averaging the gradients over the mini-batch\n",
    "           and multiplying by the learning rate\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def back_propagation(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the cost function with respect to weights and biases\n",
    "        using the backpropagation algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input to the network\n",
    "        - y: Expected output\n",
    "        \n",
    "        Returns:\n",
    "        - nabla_b: Gradient of the cost function with respect to biases\n",
    "        - nabla_w: Gradient of the cost function with respect to weights\n",
    "        \n",
    "        This method should:\n",
    "        1. Perform a feed-forward pass, storing all z values and activations\n",
    "        2. Compute the output error (difference between output and expected)\n",
    "        3. Backpropagate this error through the network\n",
    "        4. Return the gradients for biases and weights\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369cb581-08f2-47c2-8ff6-98f7c96f101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Raw Data\n",
    "    raw_training = MnistData('mnist-data/train-images.idx3-ubyte', 'mnist-data/train-labels.idx1-ubyte')\n",
    "    raw_testing = MnistData('mnist-data/t10k-images.idx3-ubyte', 'mnist-data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Processed Data\n",
    "    training = raw_training.get_data()\n",
    "    testing = raw_testing.get_data()\n",
    "\n",
    "    digit_classifier = NeuralNetwork([raw_training.img_rows * raw_training.img_cols, 15, 15, raw_training.DIGIT_COUNT])\n",
    "\n",
    "    digit_classifier.stochastic_gradient_descent(training, 10, 30, 3, testing)\n",
    "\n",
    "    accuracy = digit_classifier.evaluate(testing)\n",
    "    if accuracy:\n",
    "        print(f'Final Accuracy: {accuracy} / {len(testing)}')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32adbd-818f-41e9-952b-d2885c30e0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
