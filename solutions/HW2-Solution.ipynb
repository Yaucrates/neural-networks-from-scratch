{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc27c90-c028-4fbd-a990-b25f8b9e4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from mnistdata import MnistData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab18a96-e768-4f3f-b5da-5a935b0275d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:    \n",
    "    def __init__(self, sizes):\n",
    "        self.layer_count = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def activation_function_derivative(self, z):\n",
    "        a = self.activation_function(z)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def feed_forward(self, a):\n",
    "        for i in range(self.layer_count - 1):\n",
    "            z = self.weights[i] @ a + self.biases[i]\n",
    "            a = self.activation_function(z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        correct = 0\n",
    "        for input_data, label in test_data:\n",
    "            if self.feed_forward(input_data).argmax() == label.argmax():\n",
    "                correct += 1\n",
    "\n",
    "        return correct\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return output_activations - y\n",
    "\n",
    "    def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "            print(f\"Epoch 0: {self.evaluate(test_data)} / {n_test}\")\n",
    "            \n",
    "        n = len(training_data)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[j:j+mini_batch_size] for j in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {i+1}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {i+1} Complete!\")\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.back_propagation(x, y)\n",
    "\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.biases = [b - nb * (eta / len(mini_batch)) for b, nb in zip(self.biases, nabla_b)]\n",
    "        self.weights = [w - nw * (eta / len(mini_batch)) for w, nw in zip(self.weights, nabla_w)]\n",
    "\n",
    "    def back_propagation(self, x, y):\n",
    "        # Feed Forward to get Activations\n",
    "        layer_activations = [x]\n",
    "        layer_zs = []\n",
    "        for i in range(self.layer_count - 1):\n",
    "            z = self.weights[i] @ layer_activations[i] + self.biases[i]\n",
    "            layer_zs.append(z)\n",
    "            \n",
    "            activations = self.activation_function(z)\n",
    "            layer_activations.append(activations)\n",
    "\n",
    "        # Setup\n",
    "        error = self.cost_derivative(layer_activations[-1], y)\n",
    "        nabla_b = [None for i in range(0, self.layer_count - 1)]\n",
    "        nabla_w = [None for i in range(0, self.layer_count - 1)]\n",
    "\n",
    "        # Backpropagate Error and Calculate Change in Biases and Weights\n",
    "        for i in range(self.layer_count - 2, -1, -1):\n",
    "            error *= self.activation_function_derivative(layer_zs[i])\n",
    "            nabla_w[i] = error * np.transpose(layer_activations[i])\n",
    "            nabla_b[i] = error\n",
    "            \n",
    "            error = np.transpose(self.weights[i]) @ error\n",
    "\n",
    "        return nabla_b, nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369cb581-08f2-47c2-8ff6-98f7c96f101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Raw Data\n",
    "    raw_training = MnistData('../mnist-data/train-images.idx3-ubyte', '../mnist-data/train-labels.idx1-ubyte')\n",
    "    raw_testing = MnistData('../mnist-data/t10k-images.idx3-ubyte', '../mnist-data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Processed Data\n",
    "    training = raw_training.get_data()\n",
    "    testing = raw_testing.get_data()\n",
    "\n",
    "    digit_classifier = NeuralNetwork([raw_training.img_rows * raw_training.img_cols, 15, 15, raw_training.DIGIT_COUNT])\n",
    "\n",
    "    digit_classifier.stochastic_gradient_descent(training, 10, 30, 3, testing)\n",
    "\n",
    "    accuracy = digit_classifier.evaluate(testing)\n",
    "    print(f'Final Accuracy: {accuracy} / {len(testing)}')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
